step: 1, loss: 4.656140
step: 2, loss: 4.637950
step: 3, loss: 4.725076
step: 4, loss: 4.621604
step: 5, loss: 4.606612
step: 6, loss: 4.554296
step: 7, loss: 4.631560
step: 8, loss: 4.625556
step: 9, loss: 4.625153
step: 10, loss: 4.602942
step: 11, loss: 4.573399
step: 12, loss: 4.569832
step: 13, loss: 4.538950
step: 14, loss: 4.553937
step: 15, loss: 4.601303
step: 16, loss: 4.594291
step: 17, loss: 4.562523
step: 18, loss: 4.473553
step: 19, loss: 4.498497
step: 20, loss: 4.549232
step: 21, loss: 4.490542
step: 22, loss: 4.509385
step: 23, loss: 4.479153
step: 24, loss: 4.478247
step: 25, loss: 4.518110
step: 26, loss: 4.502932
step: 27, loss: 4.517987
step: 28, loss: 4.514594
step: 29, loss: 4.428231
step: 30, loss: 4.471825
step: 31, loss: 4.518703
step: 32, loss: 4.450556
step: 33, loss: 4.441335
step: 34, loss: 4.494578
step: 35, loss: 4.413750
step: 36, loss: 4.461250
step: 37, loss: 4.452828
step: 38, loss: 4.430993
step: 39, loss: 4.403892
step: 40, loss: 4.423824
step: 41, loss: 4.437866
step: 42, loss: 4.437551
step: 43, loss: 4.393245
step: 44, loss: 4.393778
step: 45, loss: 4.364677
step: 46, loss: 4.412146
step: 47, loss: 4.436172
step: 48, loss: 4.362261
step: 49, loss: 4.392652
step: 50, loss: 4.376635
step: 51, loss: 4.403116
step: 52, loss: 4.380639
step: 53, loss: 4.363282
step: 54, loss: 4.320452
step: 55, loss: 4.333895
step: 56, loss: 4.341104
step: 57, loss: 4.363364
step: 58, loss: 4.285352
step: 59, loss: 4.259463
step: 60, loss: 4.327191
step: 61, loss: 4.332151
step: 62, loss: 4.303633
step: 63, loss: 4.248212
step: 64, loss: 4.336387
step: 65, loss: 4.255178
step: 66, loss: 4.263855
step: 67, loss: 4.244468
step: 68, loss: 4.239466
step: 69, loss: 4.313205
step: 70, loss: 4.248563
step: 71, loss: 4.285234
step: 72, loss: 4.248964
step: 73, loss: 4.240002
step: 74, loss: 4.173879
step: 75, loss: 4.249798
step: 76, loss: 4.279533
step: 77, loss: 4.209654
step: 78, loss: 4.182946
step: 79, loss: 4.159374
step: 80, loss: 4.122478
step: 81, loss: 4.151154
step: 82, loss: 4.122067
step: 83, loss: 4.159986
step: 84, loss: 4.144706
step: 85, loss: 4.103035
step: 86, loss: 4.121578
step: 87, loss: 4.156428
step: 88, loss: 4.071676
step: 89, loss: 4.094078
step: 90, loss: 4.081265
step: 91, loss: 4.046195
step: 92, loss: 3.981534
step: 93, loss: 4.081586
step: 94, loss: 3.969927
step: 95, loss: 4.064194
step: 96, loss: 4.106261
step: 97, loss: 3.978034
step: 98, loss: 3.914027
step: 99, loss: 3.957821
step: 100, loss: 3.902672
step: 101, loss: 3.822988
step: 102, loss: 3.843429
step: 103, loss: 3.832254
step: 104, loss: 3.875763
step: 105, loss: 3.838940
step: 106, loss: 3.748573
step: 107, loss: 3.794044
step: 108, loss: 3.872880
step: 109, loss: 3.664290
step: 110, loss: 3.791957
step: 111, loss: 3.775487
step: 112, loss: 3.757242
step: 113, loss: 3.617728
step: 114, loss: 3.772348
step: 115, loss: 3.484774
step: 116, loss: 3.681959
step: 117, loss: 3.739210
step: 118, loss: 3.469457
step: 119, loss: 3.657157
step: 120, loss: 3.591041
step: 121, loss: 3.359802
step: 122, loss: 3.500422
step: 123, loss: 3.640337
step: 124, loss: 3.562362
step: 125, loss: 3.436176
step: 126, loss: 3.597709
step: 127, loss: 3.719666
step: 128, loss: 3.508530
step: 129, loss: 3.322494
step: 130, loss: 3.317779
step: 131, loss: 3.310303
step: 132, loss: 3.455145
step: 133, loss: 3.451421
step: 134, loss: 3.391183
step: 135, loss: 3.258064
step: 136, loss: 3.243866
step: 137, loss: 3.469416
step: 138, loss: 3.375462
step: 139, loss: 3.470388
step: 140, loss: 3.274562
step: 141, loss: 3.288775
step: 142, loss: 3.317638
step: 143, loss: 3.457792
step: 144, loss: 3.265077
step: 145, loss: 3.513756
step: 146, loss: 3.343681
step: 147, loss: 3.491722
step: 148, loss: 3.200295
step: 149, loss: 3.256481
step: 150, loss: 3.101398
step: 151, loss: 3.315680
step: 152, loss: 3.412781
step: 153, loss: 3.166547
step: 154, loss: 3.392667
step: 155, loss: 3.078197
step: 156, loss: 3.299467
step: 157, loss: 3.291912
step: 158, loss: 3.192703
step: 159, loss: 3.210214
step: 160, loss: 3.202269
step: 161, loss: 3.181862
step: 162, loss: 3.242446
step: 163, loss: 3.351450
step: 164, loss: 3.181280
step: 165, loss: 3.285231
step: 166, loss: 3.196109
step: 167, loss: 3.214171
step: 168, loss: 3.148063
step: 169, loss: 3.162005
step: 170, loss: 3.197078
step: 171, loss: 3.141940
step: 172, loss: 3.219948
step: 173, loss: 3.105388
step: 174, loss: 3.230068
step: 175, loss: 3.272961
step: 176, loss: 3.317247
step: 177, loss: 3.170102
step: 178, loss: 3.160788
step: 179, loss: 3.203346
step: 180, loss: 3.046477
step: 181, loss: 3.221961
step: 182, loss: 3.101218
step: 183, loss: 3.262292
step: 184, loss: 3.126030
step: 185, loss: 3.208321
step: 186, loss: 3.252460
step: 187, loss: 3.155185
step: 188, loss: 3.137871
step: 189, loss: 3.196246
step: 190, loss: 3.252732
step: 191, loss: 3.178208
step: 192, loss: 3.110595
step: 193, loss: 3.223837
step: 194, loss: 2.973836
step: 195, loss: 3.117169
step: 196, loss: 3.028113
step: 197, loss: 3.105098
step: 198, loss: 3.059020
step: 199, loss: 3.072389
step: 200, loss: 3.164483
step: 201, loss: 3.007025
step: 202, loss: 3.046586
step: 203, loss: 3.102672
step: 204, loss: 3.201328
step: 205, loss: 3.186180
step: 206, loss: 3.257474
step: 207, loss: 2.997863
step: 208, loss: 3.184546
step: 209, loss: 3.118010
step: 210, loss: 3.041840
step: 211, loss: 3.066924
step: 212, loss: 3.027142
step: 213, loss: 2.957052
step: 214, loss: 3.299807
step: 215, loss: 3.027877
step: 216, loss: 3.013489
step: 217, loss: 3.235704
step: 218, loss: 3.373156
step: 219, loss: 3.043968
step: 220, loss: 3.048858
step: 221, loss: 3.199836
step: 222, loss: 3.195886
step: 223, loss: 3.229336
step: 224, loss: 3.120985
step: 225, loss: 3.025959
step: 226, loss: 3.039353
step: 227, loss: 3.073325
step: 228, loss: 3.088725
step: 229, loss: 3.111645
step: 230, loss: 3.102798
step: 231, loss: 3.056712
step: 232, loss: 2.908343
step: 233, loss: 2.932368
step: 234, loss: 3.125695
step: 235, loss: 3.011656
step: 236, loss: 3.027593
step: 237, loss: 3.144431
step: 238, loss: 3.242396
step: 239, loss: 3.260117
step: 240, loss: 3.007915
step: 241, loss: 3.114103
step: 242, loss: 3.199450
step: 243, loss: 3.025909
step: 244, loss: 3.192897
step: 245, loss: 3.184321
step: 246, loss: 3.141529
step: 247, loss: 3.050071
step: 248, loss: 3.159666
step: 249, loss: 3.041338
step: 250, loss: 3.063801
step: 251, loss: 3.136523
step: 252, loss: 3.113335
step: 253, loss: 3.083413
step: 254, loss: 3.161942
step: 255, loss: 3.161557
step: 256, loss: 3.336378
step: 257, loss: 3.072543
step: 258, loss: 2.960437
step: 259, loss: 2.981757
step: 260, loss: 3.001443
step: 261, loss: 3.042644
step: 262, loss: 3.152106
step: 263, loss: 3.295838
step: 264, loss: 3.130752
step: 265, loss: 2.965778
step: 266, loss: 2.961716
step: 267, loss: 2.964539
step: 268, loss: 3.159070
step: 269, loss: 3.137262
step: 270, loss: 3.051803
step: 271, loss: 3.119634
step: 272, loss: 3.020989
step: 273, loss: 2.909656
step: 274, loss: 3.184737
step: 275, loss: 2.984858
step: 276, loss: 3.018014
step: 277, loss: 3.021717
step: 278, loss: 3.142519
step: 279, loss: 2.903379
step: 280, loss: 3.178612
step: 281, loss: 3.297738
step: 282, loss: 2.958723
step: 283, loss: 3.094769
step: 284, loss: 2.963341
step: 285, loss: 3.099812
step: 286, loss: 3.038297
step: 287, loss: 2.955614
step: 288, loss: 3.029122
step: 289, loss: 2.865543
step: 290, loss: 3.013189
step: 291, loss: 3.026983
step: 292, loss: 3.105703
step: 293, loss: 3.116672
step: 294, loss: 3.036013
step: 295, loss: 3.189381
step: 296, loss: 2.945281
step: 297, loss: 2.985060
step: 298, loss: 3.167711
step: 299, loss: 3.034086
step: 300, loss: 3.359151
step: 301, loss: 2.912255
step: 302, loss: 2.884242
step: 303, loss: 2.963834
step: 304, loss: 2.997504
step: 305, loss: 2.960213
step: 306, loss: 2.954088
step: 307, loss: 2.935495
step: 308, loss: 3.068447
step: 309, loss: 3.064906
step: 310, loss: 3.087125
step: 311, loss: 2.978217
step: 312, loss: 2.891361
step: 313, loss: 2.990246
step: 314, loss: 2.996840
step: 315, loss: 2.964044
step: 316, loss: 3.039440
step: 317, loss: 3.056130
step: 318, loss: 2.988335
step: 319, loss: 3.047140
step: 320, loss: 3.015730
step: 321, loss: 3.100074
step: 322, loss: 2.861289
step: 323, loss: 3.087285
step: 324, loss: 2.998063
step: 325, loss: 2.891593
step: 326, loss: 2.932975
step: 327, loss: 2.798352
step: 328, loss: 3.152219
step: 329, loss: 3.010148
step: 330, loss: 2.915415
step: 331, loss: 2.920805
step: 332, loss: 3.099041
step: 333, loss: 2.918265
step: 334, loss: 3.031978
step: 335, loss: 3.105398
step: 336, loss: 2.929737
step: 337, loss: 3.134005
step: 338, loss: 3.024129
step: 339, loss: 2.942032
step: 340, loss: 3.130395
step: 341, loss: 3.161978
step: 342, loss: 3.004207
step: 343, loss: 2.759620
step: 344, loss: 3.082481
step: 345, loss: 3.159519
step: 346, loss: 2.909188
step: 347, loss: 2.956941
step: 348, loss: 2.997206
step: 349, loss: 3.142716
step: 350, loss: 3.060081
step: 351, loss: 2.965314
step: 352, loss: 3.005257
step: 353, loss: 3.115904
step: 354, loss: 2.937036
step: 355, loss: 2.936257
step: 356, loss: 3.008093
step: 357, loss: 3.025920
step: 358, loss: 3.003518
step: 359, loss: 2.948677
step: 360, loss: 2.963129
step: 361, loss: 3.067000
step: 362, loss: 2.993454
step: 363, loss: 2.838261
step: 364, loss: 3.138077
step: 365, loss: 3.058928
step: 366, loss: 3.066276
step: 367, loss: 3.044769
step: 368, loss: 3.021350
step: 369, loss: 2.842196
step: 370, loss: 3.049740
step: 371, loss: 2.982006
step: 372, loss: 3.053108
step: 373, loss: 2.925498
step: 374, loss: 3.053083
step: 375, loss: 2.818295
step: 376, loss: 3.060934
step: 377, loss: 2.976588
step: 378, loss: 2.977046
step: 379, loss: 2.831270
step: 380, loss: 2.851147
step: 381, loss: 3.035109
step: 382, loss: 2.866289
step: 383, loss: 2.955002
step: 384, loss: 2.813798
step: 385, loss: 3.114643
step: 386, loss: 3.025272
step: 387, loss: 3.039736
step: 388, loss: 3.041063
step: 389, loss: 2.868589
step: 390, loss: 3.082597
step: 391, loss: 2.946898
step: 392, loss: 3.151960
step: 393, loss: 2.944543
step: 394, loss: 2.926906
step: 395, loss: 2.895996
step: 396, loss: 3.028148
step: 397, loss: 2.990953
step: 398, loss: 3.010150
step: 399, loss: 3.023833
step: 400, loss: 2.882295
step: 401, loss: 2.895418
step: 402, loss: 2.929538
step: 403, loss: 3.072802
step: 404, loss: 3.142930
step: 405, loss: 2.793901
step: 406, loss: 3.052269
step: 407, loss: 3.041610
step: 408, loss: 2.890748
step: 409, loss: 2.905401
step: 410, loss: 3.169902
step: 411, loss: 3.023137
step: 412, loss: 3.170235
step: 413, loss: 2.963722
step: 414, loss: 3.059589
step: 415, loss: 3.039209
step: 416, loss: 3.164666
step: 417, loss: 2.998095
step: 418, loss: 2.950308
step: 419, loss: 3.091395
step: 420, loss: 3.054157
step: 421, loss: 2.908980
step: 422, loss: 2.943789
step: 423, loss: 2.894552
step: 424, loss: 3.161664
step: 425, loss: 2.952163
step: 426, loss: 3.152940
step: 427, loss: 2.906533
step: 428, loss: 2.987530
step: 429, loss: 2.977785
step: 430, loss: 3.049402
step: 431, loss: 3.045510
step: 432, loss: 2.927713
step: 433, loss: 2.876285
step: 434, loss: 2.844456
step: 435, loss: 2.941811
step: 436, loss: 2.988751
step: 437, loss: 3.049442
step: 438, loss: 3.068800
step: 439, loss: 2.910111
step: 440, loss: 2.998441
step: 441, loss: 3.050641
step: 442, loss: 3.105310
step: 443, loss: 2.951953
step: 444, loss: 3.013901
step: 445, loss: 2.914664
step: 446, loss: 3.064645
step: 447, loss: 3.046944
step: 448, loss: 2.842279
step: 449, loss: 2.792766
step: 450, loss: 2.880464
step: 451, loss: 2.890560
step: 452, loss: 2.958614
step: 453, loss: 2.979146
step: 454, loss: 3.020093
step: 455, loss: 3.035297
step: 456, loss: 2.796045
step: 457, loss: 2.952857
step: 458, loss: 2.776442
step: 459, loss: 2.941485
step: 460, loss: 3.004863
step: 461, loss: 2.900037
step: 462, loss: 3.027680
step: 463, loss: 2.799609
step: 464, loss: 2.770047
step: 465, loss: 2.887974
step: 466, loss: 2.774309
step: 467, loss: 2.796727
step: 468, loss: 2.798439
step: 469, loss: 2.943662
step: 470, loss: 3.019495
step: 471, loss: 3.029649
step: 472, loss: 3.108134
step: 473, loss: 3.118929
step: 474, loss: 3.131421
step: 475, loss: 2.960107
step: 476, loss: 2.978732
step: 477, loss: 2.697780
step: 478, loss: 3.069297
step: 479, loss: 3.005860
step: 480, loss: 3.079629
step: 481, loss: 2.944862
step: 482, loss: 2.938220
step: 483, loss: 3.003736
step: 484, loss: 2.987269
step: 485, loss: 2.836804
step: 486, loss: 3.011193
step: 487, loss: 2.993024
step: 488, loss: 2.980503
step: 489, loss: 3.022563
step: 490, loss: 3.062261
step: 491, loss: 2.964821
step: 492, loss: 2.790027
step: 493, loss: 2.709165
step: 494, loss: 2.986190
step: 495, loss: 3.011396
step: 496, loss: 2.923796
step: 497, loss: 3.042297
step: 498, loss: 2.836395
step: 499, loss: 2.796091
step: 500, loss: 2.974201
step: 501, loss: 2.801843
step: 502, loss: 2.837394
step: 503, loss: 2.780810
step: 504, loss: 3.033307
step: 505, loss: 2.948917
step: 506, loss: 2.883142
step: 507, loss: 2.801703
step: 508, loss: 2.998976
step: 509, loss: 2.957134
step: 510, loss: 2.812502
step: 511, loss: 2.909183
step: 512, loss: 3.030397
step: 513, loss: 2.877866
step: 514, loss: 2.877046
step: 515, loss: 2.868581
step: 516, loss: 2.895356
step: 517, loss: 2.997605
step: 518, loss: 2.926388
step: 519, loss: 2.743260
step: 520, loss: 2.824383
step: 521, loss: 2.723538
step: 522, loss: 3.134028
step: 523, loss: 2.810690
step: 524, loss: 2.813743
step: 525, loss: 3.015545
step: 526, loss: 2.900690
step: 527, loss: 2.882334
step: 528, loss: 2.773679
step: 529, loss: 2.789742
step: 530, loss: 3.012595
step: 531, loss: 2.725827
step: 532, loss: 2.881078
step: 533, loss: 2.826570
step: 534, loss: 2.851555
step: 535, loss: 2.853501
step: 536, loss: 3.099020
step: 537, loss: 2.817014
step: 538, loss: 2.698550
step: 539, loss: 2.932748
step: 540, loss: 3.002900
step: 541, loss: 2.962307
step: 542, loss: 2.930426
step: 543, loss: 2.833865
step: 544, loss: 2.783995
step: 545, loss: 2.900142
step: 546, loss: 2.958914
step: 547, loss: 2.717472
step: 548, loss: 2.984191
step: 549, loss: 2.825433
step: 550, loss: 2.977542
step: 551, loss: 3.003079
step: 552, loss: 2.759124
step: 553, loss: 3.010111
step: 554, loss: 2.830806
step: 555, loss: 2.932221
step: 556, loss: 2.941515
step: 557, loss: 2.895521
step: 558, loss: 2.896537
step: 559, loss: 2.720756
step: 560, loss: 2.806374
step: 561, loss: 2.711345
step: 562, loss: 2.854363
step: 563, loss: 2.754647
step: 564, loss: 2.833074
step: 565, loss: 2.642490
step: 566, loss: 2.939750
step: 567, loss: 2.757862
step: 568, loss: 2.849068
step: 569, loss: 2.911694
step: 570, loss: 2.772687
step: 571, loss: 2.985183
step: 572, loss: 2.617980
step: 573, loss: 2.852957
step: 574, loss: 2.860674
step: 575, loss: 3.018733
step: 576, loss: 2.760071
step: 577, loss: 2.989198
step: 578, loss: 2.734759
step: 579, loss: 2.960639
step: 580, loss: 2.687507
step: 581, loss: 2.996207
step: 582, loss: 2.916740
step: 583, loss: 3.046455
step: 584, loss: 2.835891
step: 585, loss: 2.759745
step: 586, loss: 2.945878
step: 587, loss: 2.990482
step: 588, loss: 2.706267
step: 589, loss: 2.906095
step: 590, loss: 2.863612
step: 591, loss: 2.866144
step: 592, loss: 2.815251
step: 593, loss: 2.925764
step: 594, loss: 2.736629
step: 595, loss: 2.778294
step: 596, loss: 2.864758
step: 597, loss: 2.783003
step: 598, loss: 2.918187
step: 599, loss: 2.914581
step: 600, loss: 3.068364
step: 601, loss: 2.798841
step: 602, loss: 2.869615
step: 603, loss: 2.933761
step: 604, loss: 2.897403
step: 605, loss: 2.976022
step: 606, loss: 2.932188
step: 607, loss: 2.902762
step: 608, loss: 2.864733
step: 609, loss: 3.070127
step: 610, loss: 2.898079
step: 611, loss: 2.784054
step: 612, loss: 2.837766
step: 613, loss: 2.960248
step: 614, loss: 2.662963
step: 615, loss: 2.776077
step: 616, loss: 2.773551
step: 617, loss: 2.902938
step: 618, loss: 2.876631
step: 619, loss: 2.896141
step: 620, loss: 2.889242
step: 621, loss: 2.987312
step: 622, loss: 2.982287
step: 623, loss: 2.792937
step: 624, loss: 2.782269
step: 625, loss: 2.691473
step: 626, loss: 2.887170
step: 627, loss: 2.842614
step: 628, loss: 2.844136
step: 629, loss: 2.956842
step: 630, loss: 2.964749
step: 631, loss: 2.831694
step: 632, loss: 2.747697
step: 633, loss: 2.988226
step: 634, loss: 2.886637
step: 635, loss: 2.902627
step: 636, loss: 2.865793
step: 637, loss: 3.023260
step: 638, loss: 2.864155
step: 639, loss: 2.764738
step: 640, loss: 2.672897
step: 641, loss: 2.817144
step: 642, loss: 2.722011
step: 643, loss: 2.817992
step: 644, loss: 2.895066
step: 645, loss: 3.073018
step: 646, loss: 2.869095
step: 647, loss: 2.945807
step: 648, loss: 2.910813
step: 649, loss: 2.780324
step: 650, loss: 2.702567
step: 651, loss: 2.897905
step: 652, loss: 2.955437
step: 653, loss: 2.851010
step: 654, loss: 2.838099
step: 655, loss: 2.771256
step: 656, loss: 2.802154
step: 657, loss: 2.702814
step: 658, loss: 2.970921
step: 659, loss: 2.798888
step: 660, loss: 2.845203
step: 661, loss: 2.850102
step: 662, loss: 2.740054
step: 663, loss: 2.886068
step: 664, loss: 2.801893
step: 665, loss: 2.706875
step: 666, loss: 2.894336
step: 667, loss: 2.816534
step: 668, loss: 2.892593
step: 669, loss: 2.781565
step: 670, loss: 2.913840
step: 671, loss: 2.927073
step: 672, loss: 2.852180
step: 673, loss: 2.821976
step: 674, loss: 2.848980
step: 675, loss: 2.712927
step: 676, loss: 2.814221
step: 677, loss: 2.714351
step: 678, loss: 2.846122
step: 679, loss: 2.833106
step: 680, loss: 2.606605
step: 681, loss: 2.779138
step: 682, loss: 2.848526
step: 683, loss: 2.848207
step: 684, loss: 2.807359
step: 685, loss: 2.873107
step: 686, loss: 2.805624
step: 687, loss: 2.726764
step: 688, loss: 2.718232
step: 689, loss: 2.842458
step: 690, loss: 2.701978
step: 691, loss: 2.759909
step: 692, loss: 2.910801
step: 693, loss: 2.731640
step: 694, loss: 2.830139
step: 695, loss: 2.817547
step: 696, loss: 2.657323
step: 697, loss: 2.787050
step: 698, loss: 2.740712
step: 699, loss: 2.830849
step: 700, loss: 2.951298
step: 701, loss: 2.606606
step: 702, loss: 2.942155
step: 703, loss: 2.757754
step: 704, loss: 2.791605
step: 705, loss: 2.969313
step: 706, loss: 2.729343
step: 707, loss: 2.771060
step: 708, loss: 2.805256
step: 709, loss: 2.602222
step: 710, loss: 2.832583
step: 711, loss: 2.800231
step: 712, loss: 2.852885
step: 713, loss: 2.823217
step: 714, loss: 2.662604
step: 715, loss: 2.770831
step: 716, loss: 2.760537
step: 717, loss: 2.937441
step: 718, loss: 2.740051
step: 719, loss: 2.653334
step: 720, loss: 2.725656
step: 721, loss: 2.967638
step: 722, loss: 2.779597
step: 723, loss: 3.049670
step: 724, loss: 2.710044
step: 725, loss: 2.836688
step: 726, loss: 2.857122
step: 727, loss: 2.814617
step: 728, loss: 2.631200
step: 729, loss: 2.634185
step: 730, loss: 2.591856
step: 731, loss: 2.804873
step: 732, loss: 2.895202
step: 733, loss: 2.659652
step: 734, loss: 2.763962
step: 735, loss: 2.780267
step: 736, loss: 2.712122
step: 737, loss: 2.843313
step: 738, loss: 2.813586
step: 739, loss: 2.827386
step: 740, loss: 2.685991
step: 741, loss: 2.801454
step: 742, loss: 2.643604
step: 743, loss: 2.573326
step: 744, loss: 2.735464
step: 745, loss: 2.732573
step: 746, loss: 2.668998
step: 747, loss: 3.021648
step: 748, loss: 2.707033
step: 749, loss: 2.794214
step: 750, loss: 2.694453
step: 751, loss: 2.916723
step: 752, loss: 2.732605
step: 753, loss: 2.722570
step: 754, loss: 2.573852
step: 755, loss: 2.733135
step: 756, loss: 2.814003
step: 757, loss: 2.857889
step: 758, loss: 2.746310
step: 759, loss: 2.876612
step: 760, loss: 2.571179
step: 761, loss: 2.942795
step: 762, loss: 2.769582
step: 763, loss: 2.840231
step: 764, loss: 2.931401
step: 765, loss: 2.681660
step: 766, loss: 2.917035
step: 767, loss: 2.765499
step: 768, loss: 2.848364
step: 769, loss: 2.901685
step: 770, loss: 3.106787
step: 771, loss: 2.498978
step: 772, loss: 2.768503
step: 773, loss: 2.716836
step: 774, loss: 2.757587
step: 775, loss: 2.630195
step: 776, loss: 2.882888
step: 777, loss: 2.643314
step: 778, loss: 2.564927
step: 779, loss: 2.534752
step: 780, loss: 2.642116
step: 781, loss: 3.046268
step: 782, loss: 2.837528
step: 783, loss: 2.705915
step: 784, loss: 2.787399
step: 785, loss: 2.774674
step: 786, loss: 2.648422
step: 787, loss: 2.630899
step: 788, loss: 2.594704
step: 789, loss: 2.711537
step: 790, loss: 2.862541
step: 791, loss: 2.838783
step: 792, loss: 2.795702
step: 793, loss: 2.756505
step: 794, loss: 2.668777
step: 795, loss: 2.695999
step: 796, loss: 2.722611
step: 797, loss: 2.724236
step: 798, loss: 2.573824
step: 799, loss: 2.750592
step: 800, loss: 2.755020
step: 801, loss: 2.852971
step: 802, loss: 2.718476
step: 803, loss: 2.632606
step: 804, loss: 2.807375
step: 805, loss: 2.675208
step: 806, loss: 2.580271
step: 807, loss: 2.701688
step: 808, loss: 2.921974
step: 809, loss: 2.757119
step: 810, loss: 2.846043
step: 811, loss: 2.627430
step: 812, loss: 2.821582
step: 813, loss: 2.827178
step: 814, loss: 2.780565
step: 815, loss: 2.874945
step: 816, loss: 2.891669
step: 817, loss: 2.890289
step: 818, loss: 2.534757
step: 819, loss: 2.654092
step: 820, loss: 2.845089
step: 821, loss: 2.546341
step: 822, loss: 2.564154
step: 823, loss: 2.835505
step: 824, loss: 2.725245
step: 825, loss: 2.522695
step: 826, loss: 2.912553
step: 827, loss: 2.745272
step: 828, loss: 2.753075
step: 829, loss: 2.785969
step: 830, loss: 2.695318
step: 831, loss: 2.770494
step: 832, loss: 2.715970
step: 833, loss: 2.600989
step: 834, loss: 2.765717
step: 835, loss: 2.810251
step: 836, loss: 2.662150
step: 837, loss: 2.775669
step: 838, loss: 2.809561
step: 839, loss: 2.593614
step: 840, loss: 2.835902
step: 841, loss: 2.565372
step: 842, loss: 2.626630
step: 843, loss: 2.654812
step: 844, loss: 2.793800
step: 845, loss: 2.919755
step: 846, loss: 2.707565
step: 847, loss: 2.918268
step: 848, loss: 2.719027
step: 849, loss: 2.690085
step: 850, loss: 3.049097
step: 851, loss: 2.602653
step: 852, loss: 2.759735
step: 853, loss: 2.586048
step: 854, loss: 2.935804
step: 855, loss: 2.717602
step: 856, loss: 2.764087
step: 857, loss: 2.702181
step: 858, loss: 2.812656
step: 859, loss: 2.706240
step: 860, loss: 2.935391
step: 861, loss: 2.801843
step: 862, loss: 2.818065
step: 863, loss: 2.720896
step: 864, loss: 2.762110
step: 865, loss: 2.787960
step: 866, loss: 2.645086
step: 867, loss: 2.709905
step: 868, loss: 2.887757
step: 869, loss: 2.766162
step: 870, loss: 2.758518
step: 871, loss: 2.504931
step: 872, loss: 2.775956
step: 873, loss: 2.579107
step: 874, loss: 2.619372
step: 875, loss: 2.606858
step: 876, loss: 3.042958
step: 877, loss: 2.643706
step: 878, loss: 2.688370
step: 879, loss: 2.683814
step: 880, loss: 2.686816
step: 881, loss: 2.492350
step: 882, loss: 2.610872
step: 883, loss: 2.847965
step: 884, loss: 2.713956
step: 885, loss: 2.586664
step: 886, loss: 2.621422
step: 887, loss: 2.820333
step: 888, loss: 2.733493
step: 889, loss: 2.781789
step: 890, loss: 2.456486
step: 891, loss: 2.757776
step: 892, loss: 2.796989
step: 893, loss: 2.697840
step: 894, loss: 2.795711
step: 895, loss: 2.579422
step: 896, loss: 2.721411
step: 897, loss: 2.726243
step: 898, loss: 2.800763
step: 899, loss: 2.727756
step: 900, loss: 2.681707
step: 901, loss: 2.876769
step: 902, loss: 2.700446
step: 903, loss: 2.870600
step: 904, loss: 2.719373
step: 905, loss: 2.681506
step: 906, loss: 2.546515
step: 907, loss: 2.732517
step: 908, loss: 2.759188
step: 909, loss: 2.766854
step: 910, loss: 2.721884
step: 911, loss: 2.778074
step: 912, loss: 2.728340
step: 913, loss: 2.757162
step: 914, loss: 2.818343
step: 915, loss: 2.672271
step: 916, loss: 2.788663
step: 917, loss: 2.465129
step: 918, loss: 2.635178
step: 919, loss: 2.886434
step: 920, loss: 2.627810
step: 921, loss: 2.686406
step: 922, loss: 2.660235
step: 923, loss: 2.812995
step: 924, loss: 2.537171
step: 925, loss: 2.827554
step: 926, loss: 2.812229
step: 927, loss: 2.794615
step: 928, loss: 2.685897
step: 929, loss: 2.882100
step: 930, loss: 2.613160
step: 931, loss: 2.631412
step: 932, loss: 2.908489
step: 933, loss: 2.667794
step: 934, loss: 2.795570
step: 935, loss: 2.711511
step: 936, loss: 2.835835
step: 937, loss: 2.638064
step: 938, loss: 2.584604
step: 939, loss: 2.600571
step: 940, loss: 2.524158
step: 941, loss: 2.938838
step: 942, loss: 2.731302
step: 943, loss: 2.924042
step: 944, loss: 2.760940
step: 945, loss: 2.676085
step: 946, loss: 2.717554
step: 947, loss: 2.687355
step: 948, loss: 2.596468
step: 949, loss: 2.708175
step: 950, loss: 2.502316
step: 951, loss: 2.613787
step: 952, loss: 2.683426
step: 953, loss: 2.667281
step: 954, loss: 2.761668
step: 955, loss: 2.820678
step: 956, loss: 2.729213
step: 957, loss: 2.662398
step: 958, loss: 2.606617
step: 959, loss: 2.681715
step: 960, loss: 2.765074
step: 961, loss: 2.720636
step: 962, loss: 2.518847
step: 963, loss: 2.569017
step: 964, loss: 2.702120
step: 965, loss: 2.597369
step: 966, loss: 2.774297
step: 967, loss: 2.632044
step: 968, loss: 2.800348
step: 969, loss: 2.689956
step: 970, loss: 2.735698
step: 971, loss: 2.766133
step: 972, loss: 2.733299
step: 973, loss: 2.792146
step: 974, loss: 2.527222
step: 975, loss: 2.710455
step: 976, loss: 2.762589
step: 977, loss: 2.642271
step: 978, loss: 2.800487
step: 979, loss: 2.591643
step: 980, loss: 2.467056
step: 981, loss: 2.615642
step: 982, loss: 2.783134
step: 983, loss: 2.644878
step: 984, loss: 2.728431
step: 985, loss: 2.862894
step: 986, loss: 2.655892
step: 987, loss: 2.747925
step: 988, loss: 2.597795
step: 989, loss: 2.453477
step: 990, loss: 2.528048
step: 991, loss: 2.672689
step: 992, loss: 2.576463
step: 993, loss: 2.674973
step: 994, loss: 2.572750
step: 995, loss: 2.655019
step: 996, loss: 2.663654
step: 997, loss: 2.769362
step: 998, loss: 2.596473
step: 999, loss: 2.853435
