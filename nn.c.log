step: 1, loss: 4.656140
step: 2, loss: 4.637950
step: 3, loss: 4.725075
step: 4, loss: 4.621604
step: 5, loss: 4.606611
step: 6, loss: 4.554296
step: 7, loss: 4.631560
step: 8, loss: 4.625557
step: 9, loss: 4.625153
step: 10, loss: 4.602942
step: 11, loss: 4.573399
step: 12, loss: 4.569832
step: 13, loss: 4.538950
step: 14, loss: 4.553937
step: 15, loss: 4.601302
step: 16, loss: 4.594291
step: 17, loss: 4.562523
step: 18, loss: 4.473553
step: 19, loss: 4.498497
step: 20, loss: 4.549232
step: 21, loss: 4.490542
step: 22, loss: 4.509385
step: 23, loss: 4.479153
step: 24, loss: 4.478247
step: 25, loss: 4.518110
step: 26, loss: 4.502932
step: 27, loss: 4.517987
step: 28, loss: 4.514594
step: 29, loss: 4.428231
step: 30, loss: 4.471825
step: 31, loss: 4.518703
step: 32, loss: 4.450556
step: 33, loss: 4.441334
step: 34, loss: 4.494578
step: 35, loss: 4.413749
step: 36, loss: 4.461250
step: 37, loss: 4.452828
step: 38, loss: 4.430992
step: 39, loss: 4.403892
step: 40, loss: 4.423824
step: 41, loss: 4.437866
step: 42, loss: 4.437551
step: 43, loss: 4.393245
step: 44, loss: 4.393778
step: 45, loss: 4.364677
step: 46, loss: 4.412146
step: 47, loss: 4.436172
step: 48, loss: 4.362261
step: 49, loss: 4.392652
step: 50, loss: 4.376635
step: 51, loss: 4.403116
step: 52, loss: 4.380639
step: 53, loss: 4.363282
step: 54, loss: 4.320452
step: 55, loss: 4.333895
step: 56, loss: 4.341103
step: 57, loss: 4.363364
step: 58, loss: 4.285352
step: 59, loss: 4.259463
step: 60, loss: 4.327191
step: 61, loss: 4.332151
step: 62, loss: 4.303633
step: 63, loss: 4.248212
step: 64, loss: 4.336386
step: 65, loss: 4.255178
step: 66, loss: 4.263856
step: 67, loss: 4.244468
step: 68, loss: 4.239466
step: 69, loss: 4.313205
step: 70, loss: 4.248563
step: 71, loss: 4.285234
step: 72, loss: 4.248964
step: 73, loss: 4.240002
step: 74, loss: 4.173880
step: 75, loss: 4.249798
step: 76, loss: 4.279533
step: 77, loss: 4.209654
step: 78, loss: 4.182946
step: 79, loss: 4.159374
step: 80, loss: 4.122478
step: 81, loss: 4.151154
step: 82, loss: 4.122067
step: 83, loss: 4.159985
step: 84, loss: 4.144706
step: 85, loss: 4.103035
step: 86, loss: 4.121578
step: 87, loss: 4.156428
step: 88, loss: 4.071676
step: 89, loss: 4.094078
step: 90, loss: 4.081265
step: 91, loss: 4.046195
step: 92, loss: 3.981534
step: 93, loss: 4.081586
step: 94, loss: 3.969927
step: 95, loss: 4.064194
step: 96, loss: 4.106262
step: 97, loss: 3.978033
step: 98, loss: 3.914027
step: 99, loss: 3.957821
step: 100, loss: 3.902672
step: 101, loss: 3.822988
step: 102, loss: 3.843429
step: 103, loss: 3.832254
step: 104, loss: 3.875762
step: 105, loss: 3.838940
step: 106, loss: 3.748573
step: 107, loss: 3.794044
step: 108, loss: 3.872880
step: 109, loss: 3.664290
step: 110, loss: 3.791957
step: 111, loss: 3.775486
step: 112, loss: 3.757241
step: 113, loss: 3.617728
step: 114, loss: 3.772347
step: 115, loss: 3.484774
step: 116, loss: 3.681959
step: 117, loss: 3.739210
step: 118, loss: 3.469457
step: 119, loss: 3.657157
step: 120, loss: 3.591041
step: 121, loss: 3.359802
step: 122, loss: 3.500422
step: 123, loss: 3.640337
step: 124, loss: 3.562362
step: 125, loss: 3.436176
step: 126, loss: 3.597709
step: 127, loss: 3.719666
step: 128, loss: 3.508530
step: 129, loss: 3.322494
step: 130, loss: 3.317780
step: 131, loss: 3.310303
step: 132, loss: 3.455145
step: 133, loss: 3.451421
step: 134, loss: 3.391183
step: 135, loss: 3.258063
step: 136, loss: 3.243866
step: 137, loss: 3.469416
step: 138, loss: 3.375462
step: 139, loss: 3.470388
step: 140, loss: 3.274562
step: 141, loss: 3.288775
step: 142, loss: 3.317638
step: 143, loss: 3.457792
step: 144, loss: 3.265077
step: 145, loss: 3.513757
step: 146, loss: 3.343681
step: 147, loss: 3.491722
step: 148, loss: 3.200295
step: 149, loss: 3.256481
step: 150, loss: 3.101398
step: 151, loss: 3.315680
step: 152, loss: 3.412781
step: 153, loss: 3.166547
step: 154, loss: 3.392667
step: 155, loss: 3.078197
step: 156, loss: 3.299467
step: 157, loss: 3.291912
step: 158, loss: 3.192703
step: 159, loss: 3.210213
step: 160, loss: 3.202269
step: 161, loss: 3.181863
step: 162, loss: 3.242446
step: 163, loss: 3.351450
step: 164, loss: 3.181280
step: 165, loss: 3.285232
step: 166, loss: 3.196109
step: 167, loss: 3.214170
step: 168, loss: 3.148063
step: 169, loss: 3.162005
step: 170, loss: 3.197078
step: 171, loss: 3.141940
step: 172, loss: 3.219947
step: 173, loss: 3.105388
step: 174, loss: 3.230068
step: 175, loss: 3.272961
step: 176, loss: 3.317247
step: 177, loss: 3.170102
step: 178, loss: 3.160788
step: 179, loss: 3.203346
step: 180, loss: 3.046477
step: 181, loss: 3.221961
step: 182, loss: 3.101218
step: 183, loss: 3.262292
step: 184, loss: 3.126030
step: 185, loss: 3.208321
step: 186, loss: 3.252460
step: 187, loss: 3.155184
step: 188, loss: 3.137872
step: 189, loss: 3.196247
step: 190, loss: 3.252732
step: 191, loss: 3.178208
step: 192, loss: 3.110595
step: 193, loss: 3.223837
step: 194, loss: 2.973836
step: 195, loss: 3.117169
step: 196, loss: 3.028113
step: 197, loss: 3.105098
step: 198, loss: 3.059020
step: 199, loss: 3.072389
step: 200, loss: 3.164482
step: 201, loss: 3.007024
step: 202, loss: 3.046586
step: 203, loss: 3.102672
step: 204, loss: 3.201329
step: 205, loss: 3.186180
step: 206, loss: 3.257474
step: 207, loss: 2.997864
step: 208, loss: 3.184546
step: 209, loss: 3.118010
step: 210, loss: 3.041840
step: 211, loss: 3.066924
step: 212, loss: 3.027142
step: 213, loss: 2.957052
step: 214, loss: 3.299807
step: 215, loss: 3.027877
step: 216, loss: 3.013489
step: 217, loss: 3.235704
step: 218, loss: 3.373156
step: 219, loss: 3.043968
step: 220, loss: 3.048858
step: 221, loss: 3.199836
step: 222, loss: 3.195886
step: 223, loss: 3.229336
step: 224, loss: 3.120985
step: 225, loss: 3.025959
step: 226, loss: 3.039353
step: 227, loss: 3.073325
step: 228, loss: 3.088725
step: 229, loss: 3.111645
step: 230, loss: 3.102798
step: 231, loss: 3.056712
step: 232, loss: 2.908343
step: 233, loss: 2.932368
step: 234, loss: 3.125695
step: 235, loss: 3.011656
step: 236, loss: 3.027593
step: 237, loss: 3.144431
step: 238, loss: 3.242396
step: 239, loss: 3.260117
step: 240, loss: 3.007915
step: 241, loss: 3.114103
step: 242, loss: 3.199450
step: 243, loss: 3.025909
step: 244, loss: 3.192897
step: 245, loss: 3.184321
step: 246, loss: 3.141529
step: 247, loss: 3.050072
step: 248, loss: 3.159666
step: 249, loss: 3.041338
step: 250, loss: 3.063802
step: 251, loss: 3.136523
step: 252, loss: 3.113335
step: 253, loss: 3.083413
step: 254, loss: 3.161942
step: 255, loss: 3.161557
step: 256, loss: 3.336378
step: 257, loss: 3.072543
step: 258, loss: 2.960437
step: 259, loss: 2.981757
step: 260, loss: 3.001443
step: 261, loss: 3.042644
step: 262, loss: 3.152106
step: 263, loss: 3.295838
step: 264, loss: 3.130752
step: 265, loss: 2.965778
step: 266, loss: 2.961716
step: 267, loss: 2.964539
step: 268, loss: 3.159070
step: 269, loss: 3.137263
step: 270, loss: 3.051803
step: 271, loss: 3.119634
step: 272, loss: 3.020989
step: 273, loss: 2.909656
step: 274, loss: 3.184737
step: 275, loss: 2.984858
step: 276, loss: 3.018014
step: 277, loss: 3.021717
step: 278, loss: 3.142519
step: 279, loss: 2.903379
step: 280, loss: 3.178612
step: 281, loss: 3.297738
step: 282, loss: 2.958723
step: 283, loss: 3.094769
step: 284, loss: 2.963341
step: 285, loss: 3.099812
step: 286, loss: 3.038298
step: 287, loss: 2.955614
step: 288, loss: 3.029122
step: 289, loss: 2.865544
step: 290, loss: 3.013189
step: 291, loss: 3.026983
step: 292, loss: 3.105703
step: 293, loss: 3.116672
step: 294, loss: 3.036013
step: 295, loss: 3.189380
step: 296, loss: 2.945281
step: 297, loss: 2.985060
step: 298, loss: 3.167711
step: 299, loss: 3.034086
step: 300, loss: 3.359152
step: 301, loss: 2.912255
step: 302, loss: 2.884242
step: 303, loss: 2.963834
step: 304, loss: 2.997504
step: 305, loss: 2.960213
step: 306, loss: 2.954088
step: 307, loss: 2.935495
step: 308, loss: 3.068447
step: 309, loss: 3.064906
step: 310, loss: 3.087125
step: 311, loss: 2.978216
step: 312, loss: 2.891361
step: 313, loss: 2.990246
step: 314, loss: 2.996840
step: 315, loss: 2.964044
step: 316, loss: 3.039440
step: 317, loss: 3.056130
step: 318, loss: 2.988336
step: 319, loss: 3.047139
step: 